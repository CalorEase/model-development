{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import logging\n",
    "import re\n",
    "import shutil\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tarfile\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "\n",
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf1\n",
    "import contextlib2\n",
    "import tensorflow as tf\n",
    "import wget\n",
    "\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.utils import label_map_util\n",
    "# from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "# from object_detection.utils import colab_utils\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.dataset_tools import tf_record_creation_util\n",
    "\n",
    "\n",
    "# Enable GPU dynamic memory allocation\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINK : http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz\n",
    "model_name = \"efficientdet_d0_coco17_tpu-32\"\n",
    "model = \"efficientdet_d0_coco17_tpu-32.tar.gz\"\n",
    "\n",
    "# os.makedirs(\"pre_trained_models/\", exist_ok=True)\n",
    "download_tar = f\"http://download.tensorflow.org/models/object_detection/tf2/20200711/{model}\"\n",
    "wget.download(download_tar, out=\"pre_trained_models/\")\n",
    "\n",
    "tar = tarfile.open(f\"pre_trained_models/{model}\")\n",
    "tar.extractall(path=\"pre_trained_models/\")\n",
    "tar.close()\n",
    "\n",
    "os.unlink(f\"pre_trained_models/{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pipeline_path = \"pre_trained_models/pipeline_olahan_daging.config\"\n",
    "fine_tune_checkpoint = \"pre_trained_models/efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0\"\n",
    "train_input_path = \"dataset/olahan_daging/train/food.tfrecord\"\n",
    "valid_input_path = \"dataset/olahan_daging/valid/food.tfrecord\"\n",
    "label_map_path = \"dataset/olahan_daging/train/food_label_map.pbtxt\"\n",
    "num_classes = 6\n",
    "train_batch_size = 8\n",
    "min_dim = 256\n",
    "max_dim = 256\n",
    "\n",
    "with open(pipeline_path) as f:\n",
    "    pipeline_content = f.read()\n",
    "with open(pipeline_path, 'w') as f:\n",
    "    # change to finetune checkpoint path\n",
    "    pipeline_content = re.sub('fine_tune_checkpoint: \".*?\"',\n",
    "               f'fine_tune_checkpoint: \"{fine_tune_checkpoint}\"', pipeline_content)\n",
    "\n",
    "    # change to tfrecord path for train and validation\n",
    "    pipeline_content = re.sub(\n",
    "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', f'input_path: \"{train_input_path}\"', pipeline_content)\n",
    "    pipeline_content = re.sub(\n",
    "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', f'input_path: \"{valid_input_path}\"', pipeline_content)\n",
    "\n",
    "    # change to label_map_path\n",
    "    pipeline_content = re.sub(\n",
    "        'label_map_path: \".*?\"', f'label_map_path: \"{label_map_path}\"', pipeline_content)\n",
    "\n",
    "    # Set training batch_size\n",
    "    pipeline_content = re.sub(r'(train_config\\s*{\\s*batch_size:\\s*)\\d+', r'\\g<1>{}'.format(train_batch_size), pipeline_content)\n",
    "\n",
    "    # Set the dimension\n",
    "    pipeline_content = re.sub('min_dimension: [0-9]+',\n",
    "               f'min_dimension: {min_dim}', pipeline_content)\n",
    "    pipeline_content = re.sub('max_dimension: [0-9]+',\n",
    "               f'max_dimension: {max_dim}', pipeline_content)\n",
    "\n",
    "    # Set number of classes num_classes.\n",
    "    pipeline_content = re.sub('num_classes: [0-9]+',\n",
    "               f'num_classes: {num_classes}', pipeline_content)\n",
    "\n",
    "    # change the finetune type to detection\n",
    "    pipeline_content = re.sub(\n",
    "        'fine_tune_checkpoint_type: \"classification\"', f'fine_tune_checkpoint_type: \"{\"detection\"}\"', pipeline_content)\n",
    "\n",
    "    f.write(pipeline_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#run on terminal\n",
    "\n",
    "!python models/research/object_detection/model_main_tf2.py --pipeline_config_path=pre_trained_models/pipeline_olahan_daging.config --num_train_steps=20000 --model_dir=training/olahan_daging_efficientdet_d0 --alsologtostderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#run on terminal with above so mAP score shows\n",
    "\n",
    "!python models/research/object_detection/model_main_tf2.py --pipeline_config_path=pre_trained_models/pipeline_olahan_daging.config --model_dir=training/olahan_daging_efficientdet_d0 --checkpoint_dir=training/olahan_daging_efficientdet_d0 --alsologtostderr --eval_timeout=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorboard --logdir=training/olahan_daging_efficientdet_d0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#run on terminal\n",
    "\n",
    "!python models/research/object_detection/exporter_main_v2.py --trained_checkpoint_dir=training/olahan_daging_efficientdet_d0 --output_directory=custom_model_lite/olahan_daging_efficientdet_d0 --pipeline_config_path=pre_trained_models/pipeline_olahan_daging.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ayam bakar': {1: {'id': 1, 'name': 'ayam bakar'}}, 'ayam goreng': {2: {'id': 2, 'name': 'ayam goreng'}}, 'ikan goreng': {3: {'id': 3, 'name': 'ikan goreng'}}, 'sate': {4: {'id': 4, 'name': 'sate'}}, 'steak': {5: {'id': 5, 'name': 'steak'}}}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample data string\n",
    "data = \"\"\"\n",
    "item {\n",
    "  name: \"ayam bakar\"\n",
    "  id: 1\n",
    "  display_name: \"ayam bakar\"\n",
    "}\n",
    "item {\n",
    "  name: \"ayam goreng\"\n",
    "  id: 2\n",
    "  display_name: \"ayam goreng\"\n",
    "}\n",
    "item {\n",
    "  name: \"ikan goreng\"\n",
    "  id: 3\n",
    "  display_name: \"ikan goreng\"\n",
    "}\n",
    "item {\n",
    "  name: \"sate\"\n",
    "  id: 4\n",
    "  display_name: \"sate\"\n",
    "}\n",
    "item {\n",
    "  name: \"steak\"\n",
    "  id: 5\n",
    "  display_name: \"steak\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Define the regular expression patterns for name, id, and display_name\n",
    "# name_pattern = r'name:\\s*\"([^\"]*)\"'\n",
    "id_pattern = r'id:\\s*(\\d+)'\n",
    "display_name_pattern = r'display_name:\\s*\"([^\"]*)\"'\n",
    "\n",
    "# Find all matches using regex\n",
    "# names = re.findall(name_pattern, data)\n",
    "ids = [int(i) for i in re.findall(id_pattern, data)]\n",
    "display_names = re.findall(display_name_pattern, data)\n",
    "\n",
    "# Structure the data as required\n",
    "result = {}\n",
    "for i in range(len(display_names)):\n",
    "    result[display_names[i]] = {ids[i]: {'id': ids[i], 'name': display_names[i]}}\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ayam bakar',\n",
       " 'ayam bakar',\n",
       " 'ayam goreng',\n",
       " 'ayam goreng',\n",
       " 'ikan goreng',\n",
       " 'ikan goreng',\n",
       " 'sate',\n",
       " 'sate',\n",
       " 'steak',\n",
       " 'steak']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cat = [p.split(\"_efficientdet_d0\")[0] for p in os.listdir(\"custom_model_lite\")]\n",
    "# models = {}\n",
    "category_indexs = {}\n",
    "\n",
    "def read_label_file(label_path):\n",
    "    id_pattern = r'id:\\s*(\\d+)'\n",
    "    display_name_pattern = r'display_name:\\s*\"([^\"]*)\"'\n",
    "    with open(label_path, 'r') as file:\n",
    "        pbtxt_content = file.read()\n",
    "        ids = [int(i) for i in re.findall(id_pattern,  pbtxt_content)]\n",
    "        display_names = re.findall(display_name_pattern, pbtxt_content)\n",
    "    result = {}\n",
    "    for i in range(len(display_names)):\n",
    "        result[ids[i]] = {'id': ids[i], 'name': display_names[i]}\n",
    "    return result\n",
    "\n",
    "for cat in model_cat:\n",
    "    # models[cat] = tf.saved_model.load(f\"custom_model_lite\\{cat}_efficientdet_d0\\saved_model\")\n",
    "    category_indexs[cat] = read_label_file(f\"dataset/{cat}/train/food_label_map.pbtxt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'daging': {1: {'id': 1, 'name': 'ayam bakar'},\n",
       "  2: {'id': 2, 'name': 'ayam goreng'},\n",
       "  3: {'id': 3, 'name': 'ikan goreng'},\n",
       "  4: {'id': 4, 'name': 'sate'},\n",
       "  5: {'id': 5, 'name': 'steak'}},\n",
       " 'jajanan': {1: {'id': 1, 'name': 'bakwan'},\n",
       "  2: {'id': 2, 'name': 'batagor'},\n",
       "  3: {'id': 3, 'name': 'bika_ambon'},\n",
       "  4: {'id': 4, 'name': 'martabak telur'},\n",
       "  5: {'id': 5, 'name': 'pempek'}},\n",
       " 'karbo': {1: {'id': 1, 'name': 'bihun'},\n",
       "  2: {'id': 2, 'name': 'mie'},\n",
       "  3: {'id': 3, 'name': 'nasi goreng'},\n",
       "  4: {'id': 4, 'name': 'nasi putih'},\n",
       "  5: {'id': 5, 'name': 'roti'}},\n",
       " 'lauk': {1: {'id': 1, 'name': 'kerupuk'},\n",
       "  2: {'id': 2, 'name': 'tahu'},\n",
       "  3: {'id': 3, 'name': 'telur'},\n",
       "  4: {'id': 4, 'name': 'tempe'}},\n",
       " 'sayur': {1: {'id': 1, 'name': 'capcay'},\n",
       "  2: {'id': 2, 'name': 'gado-gado'},\n",
       "  3: {'id': 3, 'name': 'soto'},\n",
       "  4: {'id': 4, 'name': 'terong balado'},\n",
       "  5: {'id': 5, 'name': 'tumis kangkung'}}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'daging': {1: {'id': 1, 'name': 'ayam bakar'},\n",
       "  2: {'id': 2, 'name': 'ayam goreng'},\n",
       "  3: {'id': 3, 'name': 'ikan goreng'},\n",
       "  4: {'id': 4, 'name': 'sate'},\n",
       "  5: {'id': 5, 'name': 'steak'}},\n",
       " 'jajanan': {1: {'id': 1, 'name': 'bakwan'},\n",
       "  2: {'id': 2, 'name': 'batagor'},\n",
       "  3: {'id': 3, 'name': 'bika_ambon'},\n",
       "  4: {'id': 4, 'name': 'martabak telur'},\n",
       "  5: {'id': 5, 'name': 'pempek'}},\n",
       " 'karbo': {1: {'id': 1, 'name': 'bihun'},\n",
       "  2: {'id': 2, 'name': 'mie'},\n",
       "  3: {'id': 3, 'name': 'nasi goreng'},\n",
       "  4: {'id': 4, 'name': 'nasi putih'},\n",
       "  5: {'id': 5, 'name': 'roti'}},\n",
       " 'lauk': {1: {'id': 1, 'name': 'kerupuk'},\n",
       "  2: {'id': 2, 'name': 'tahu'},\n",
       "  3: {'id': 3, 'name': 'telur'},\n",
       "  4: {'id': 4, 'name': 'tempe'}},\n",
       " 'sayur': {1: {'id': 1, 'name': 'capcay'},\n",
       "  2: {'id': 2, 'name': 'gado-gado'},\n",
       "  3: {'id': 3, 'name': 'soto'},\n",
       "  4: {'id': 4, 'name': 'terong balado'},\n",
       "  5: {'id': 5, 'name': 'tumis kangkung'}}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_detected = []\n",
    "IMAGE_PATHS = \"testing/nasi_ikan.jpg\"\n",
    "\n",
    "image_np = np.array(Image.open(IMAGE_PATHS))\n",
    "input_tensor = tf.convert_to_tensor(image_np, dtype=tf.uint8)\n",
    "input_tensor = input_tensor[tf.newaxis, ...]\n",
    "input_tensor = input_tensor[:, :, :, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in model_cat:\n",
    "    detections = models[cat]((input_tensor))\n",
    "    classes = detections['detection_classes'][0].numpy()\n",
    "    scores = detections['detection_scores'][0].numpy()\n",
    "    for i in range(len(scores)):\n",
    "        if((scores[i] > 0.5) and (scores[i] <= 1.0)):\n",
    "            object_name = category_indexs[cat][classes[i]]\n",
    "            obj_detected.append(object_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'daging': {1: {'id': 1, 'name': 'ayam bakar'},\n",
       "  2: {'id': 2, 'name': 'ayam goreng'},\n",
       "  3: {'id': 3, 'name': 'ikan goreng'},\n",
       "  4: {'id': 4, 'name': 'sate'},\n",
       "  5: {'id': 5, 'name': 'steak'}},\n",
       " 'jajanan': {1: {'id': 1, 'name': 'bakwan'},\n",
       "  2: {'id': 2, 'name': 'batagor'},\n",
       "  3: {'id': 3, 'name': 'bika_ambon'},\n",
       "  4: {'id': 4, 'name': 'martabak telur'},\n",
       "  5: {'id': 5, 'name': 'pempek'}},\n",
       " 'karbo': {1: {'id': 1, 'name': 'bihun'},\n",
       "  2: {'id': 2, 'name': 'mie'},\n",
       "  3: {'id': 3, 'name': 'nasi goreng'},\n",
       "  4: {'id': 4, 'name': 'nasi putih'},\n",
       "  5: {'id': 5, 'name': 'roti'}},\n",
       " 'lauk': {1: {'id': 1, 'name': 'kerupuk'},\n",
       "  2: {'id': 2, 'name': 'tahu'},\n",
       "  3: {'id': 3, 'name': 'telur'},\n",
       "  4: {'id': 4, 'name': 'tempe'}},\n",
       " 'sayur': {1: {'id': 1, 'name': 'capcay'},\n",
       "  2: {'id': 2, 'name': 'gado-gado'},\n",
       "  3: {'id': 3, 'name': 'soto'},\n",
       "  4: {'id': 4, 'name': 'terong balado'},\n",
       "  5: {'id': 5, 'name': 'tumis kangkung'}}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ikan goreng', 'nasi putih']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/olahan_daging/train/food_label_map.pbtxt', 'r') as file:\n",
    "    pbtxt_content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bakso', 'nugget', 'opor ayam', 'rendang', 'sosis', 'udang']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'display_name:\\s*\"([^\"]*)\"'\n",
    "display_names = re.findall(pattern, pbtxt_content)\n",
    "display_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_detected = []\n",
    "IMAGE_PATHS = \"nasi_ikan.jpg\"\n",
    "\n",
    "for p in os.listdir(\"custom_model_lite\"):\n",
    "    cat = p.split(\"_efficientdet_d0\")[0]\n",
    "    saved_model_loaded = tf.saved_model.load(f\"custom_model_lite\\{p}\\saved_model\")\n",
    "    category_index = label_map_util.create_category_index_from_labelmap(f\"{cat}/train/food_label_map.pbtxt\", use_display_name=True)\n",
    "    image_np = np.array(Image.open(IMAGE_PATHS))\n",
    "    input_tensor = tf.convert_to_tensor(image_np, dtype=tf.uint8)\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "    input_tensor = input_tensor[:, :, :, :3]\n",
    "    detections = saved_model_loaded(input_tensor)\n",
    "    classes = detections['detection_classes'][0].numpy()\n",
    "    scores = detections['detection_scores'][0].numpy()\n",
    "    for i in range(len(scores)):\n",
    "        if((scores[i] > 0.5) and (scores[i] <= 1.0)):\n",
    "            object_name = category_index[classes[i]]['name']\n",
    "            obj_detected.append(object_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ikan goreng', 'nasi putih']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_detected = []\n",
    "\n",
    "for d in detections_array:\n",
    "    classes = d['detection_classes'][0].numpy()\n",
    "    scores = d['detection_scores'][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_loaded = tf.saved_model.load(\"custom_model_lite\\jajanan_efficientdet_d0\\saved_model\")\n",
    "PATH_TO_LABELS = 'jajanan/train/food_label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
    "\n",
    "IMAGE_PATHS = \"photo.jpg\"\n",
    "image_np = np.array(Image.open(IMAGE_PATHS))\n",
    "input_tensor = tf.convert_to_tensor(image_np, dtype=tf.uint8)\n",
    "input_tensor = input_tensor[tf.newaxis, ...]\n",
    "input_tensor = input_tensor[:, :, :, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bakwan'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_index[1]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 400, 400, 3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_np = np.array(Image.open(IMAGE_PATHS))\n",
    "input_tensor = tf.convert_to_tensor(image_np, dtype=tf.uint8)\n",
    "input_tensor = input_tensor[tf.newaxis, ...]\n",
    "input_tensor = input_tensor[:, :, :, :3]\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = saved_model_loaded(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = detections['detection_classes'][0].numpy()\n",
    "scores = detections['detection_scores'][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 3., 2., 5., 3., 1., 5., 5., 2., 1., 1., 2., 5., 3., 5., 2., 5.,\n",
       "       5., 2., 1., 2., 2., 5., 3., 5., 2., 2., 3., 3., 1., 5., 5., 5., 2.,\n",
       "       2., 2., 5., 1., 5., 1., 5., 5., 5., 2., 3., 2., 5., 5., 1., 5., 5.,\n",
       "       1., 1., 5., 1., 3., 4., 5., 2., 4., 5., 1., 2., 3., 2., 2., 1., 1.,\n",
       "       5., 3., 4., 1., 5., 5., 2., 1., 5., 2., 3., 4., 2., 1., 5., 5., 2.,\n",
       "       5., 3., 5., 1., 5., 1., 3., 3., 2., 4., 3., 2., 5., 1., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01394221, 0.01222091, 0.01155228, 0.01058487, 0.01026587,\n",
       "        0.01023106, 0.00995212, 0.00923526, 0.00905502, 0.00875791,\n",
       "        0.00850109, 0.00847665, 0.00838382, 0.00838023, 0.00818047,\n",
       "        0.00780999, 0.00756392, 0.00756196, 0.00752028, 0.00749922,\n",
       "        0.0074649 , 0.00736557, 0.00733423, 0.00731031, 0.00721338,\n",
       "        0.00703817, 0.00686638, 0.00679421, 0.00676177, 0.00675764,\n",
       "        0.00656289, 0.00653557, 0.00644133, 0.00639698, 0.00638087,\n",
       "        0.00636783, 0.0062957 , 0.00629158, 0.00626436, 0.00622538,\n",
       "        0.00619953, 0.00603963, 0.00602442, 0.00600811, 0.00597259,\n",
       "        0.00594399, 0.00590838, 0.00587545, 0.00586731, 0.00585866,\n",
       "        0.00583623, 0.0057706 , 0.00576108, 0.00576054, 0.00573587,\n",
       "        0.0057319 , 0.00570947, 0.00570738, 0.0056954 , 0.00567021,\n",
       "        0.0056359 , 0.00563022, 0.0056287 , 0.00562349, 0.00559751,\n",
       "        0.00559558, 0.00551532, 0.00550622, 0.0054363 , 0.00543624,\n",
       "        0.00540517, 0.0053587 , 0.00533089, 0.00531841, 0.00527298,\n",
       "        0.00526197, 0.00522473, 0.00520627, 0.00518227, 0.00516308,\n",
       "        0.00516071, 0.00515733, 0.00511205, 0.00508819, 0.0050194 ,\n",
       "        0.00501353, 0.00496156, 0.00495919, 0.00495586, 0.00495078,\n",
       "        0.00492758, 0.00491132, 0.0048726 , 0.00486883, 0.00486214,\n",
       "        0.0048244 , 0.00474615, 0.00474002, 0.00473392, 0.00470721]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_detected = []\n",
    "for i in range(len(scores[0])):\n",
    "    if((scores[0][i] > 0.5) and (scores[0][i] <= 1.0)):\n",
    "        object_name = category_index[classes[0][i]]['name']\n",
    "        obj_detected.append(object_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
